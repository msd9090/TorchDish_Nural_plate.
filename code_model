{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":15282,"databundleVersionId":565187,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mah20050/my-python-s-pytorch-1?scriptVersionId=262665034\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# This project for classifying dirty Dishes and cleaned Dishes \n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:42.279676Z","iopub.execute_input":"2025-09-18T20:19:42.280172Z","iopub.status.idle":"2025-09-18T20:19:42.291546Z","shell.execute_reply.started":"2025-09-18T20:19:42.280125Z","shell.execute_reply":"2025-09-18T20:19:42.290094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# order libraries\nimport torch \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport time \nimport copy \nimport zipfile \nimport shutil \nfrom tqdm import tqdm \nfrom torchvision import transforms , models \nimport torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:42.293922Z","iopub.execute_input":"2025-09-18T20:19:42.294361Z","iopub.status.idle":"2025-09-18T20:19:42.322885Z","shell.execute_reply.started":"2025-09-18T20:19:42.294318Z","shell.execute_reply":"2025-09-18T20:19:42.321479Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 1.Photo modification","metadata":{}},{"cell_type":"code","source":"# Unzip photos\nwith zipfile.ZipFile('../input/platesv2/plates.zip','r')as file: \n    file.extractall('/kaggle/working') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:42.324405Z","iopub.execute_input":"2025-09-18T20:19:42.324713Z","iopub.status.idle":"2025-09-18T20:19:42.716549Z","shell.execute_reply.started":"2025-09-18T20:19:42.324691Z","shell.execute_reply":"2025-09-18T20:19:42.715368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = '/kaggle/working/plates/' \nprint(os.listdir(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:42.719151Z","iopub.execute_input":"2025-09-18T20:19:42.719537Z","iopub.status.idle":"2025-09-18T20:19:42.727395Z","shell.execute_reply.started":"2025-09-18T20:19:42.719503Z","shell.execute_reply":"2025-09-18T20:19:42.725885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# chapter about different types of photos Dishes\n\ntrain_uncl = 'train' \nval_uncl = 'val' \n\ntype_names = ['cleaned','dirty'] \n\nfor uncl_name in [train_uncl , val_uncl]:\n    for type_name in type_names:\n        os.makedirs(os.path.join(uncl_name , type_name),exist_ok =True) \n\nfor type_name in type_names:\n    source_uncl = os.path.join(data,'train',type_name) \n    for i , infor_name in enumerate(tqdm(os.listdir(source_uncl))):\n        if i % 6 != 0:\n            dest_uncl = os.path.join(train_uncl , type_name) \n        else:\n            dest_uncl = os.path.join(val_uncl , type_name) \n        shutil.copy(os.path.join(source_uncl , infor_name),os.path.join(dest_uncl , infor_name))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:42.728674Z","iopub.execute_input":"2025-09-18T20:19:42.728987Z","iopub.status.idle":"2025-09-18T20:19:42.773916Z","shell.execute_reply.started":"2025-09-18T20:19:42.728962Z","shell.execute_reply":"2025-09-18T20:19:42.77192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2.building Torch's model for size photos","metadata":{}},{"cell_type":"code","source":"train_trans= transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) \n    \n])\nval_trans = transforms.Compose([\n    transforms.Resize((244,244)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_uncl , train_trans) \nval_dataset = torchvision.datasets.ImageFolder(val_uncl,val_trans) \n\nbatch_size = 8 \n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset , batch_size = batch_size , shuffle = True  , num_workers = batch_size) \n\nval_dataloader = torch.utils.data.DataLoader(val_dataset , batch_size = batch_size , shuffle = False , num_workers = batch_size) \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:42.775679Z","iopub.execute_input":"2025-09-18T20:19:42.776078Z","iopub.status.idle":"2025-09-18T20:19:42.788264Z","shell.execute_reply.started":"2025-09-18T20:19:42.776041Z","shell.execute_reply":"2025-09-18T20:19:42.787006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataloader), len(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:42.789395Z","iopub.execute_input":"2025-09-18T20:19:42.789671Z","iopub.status.idle":"2025-09-18T20:19:42.822599Z","shell.execute_reply.started":"2025-09-18T20:19:42.789651Z","shell.execute_reply":"2025-09-18T20:19:42.820938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.Drawing pictures","metadata":{}},{"cell_type":"code","source":"def input_photos(input_tensor , title = ''):\n    \n    image = input_tensor.permute(1,2,0).numpy() \n    image = std * image + mean \n    plt.imshow(image.clip(0,1))\n    plt.title(title)\n    plt.show() \n    plt.pause(0.001) \n\nX_batch , y_batch = next(iter(train_dataloader)) \nmean = np.array([0.485 , 0.456 , 0.406]) \nstd = np.array([0.229 , 0.224,0.225]) \n\n\nfor x_item , y_item in zip(X_batch , y_batch):\n    input_photos(x_item , title = type_names[y_item])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:42.824571Z","iopub.execute_input":"2025-09-18T20:19:42.825348Z","iopub.status.idle":"2025-09-18T20:19:45.797912Z","shell.execute_reply.started":"2025-09-18T20:19:42.825302Z","shell.execute_reply":"2025-09-18T20:19:45.79657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. PyTorch model","metadata":{}},{"cell_type":"code","source":"def train_model(model , loss , optimizer , rules , num_epochs):\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}:'.format(epoch , num_epochs - 1), flush = True)\n\n\n        for x in ['train','val']:\n            if x == 'train':\n                dataloader = train_dataloader\n                rules.step() \n                model.train() \n            else:\n                dataloader = val_dataloader\n                model.eval() \n\n            running_loss = 0.\n            running_acc = 0. \n\n            for inputs , labels in tqdm(dataloader):\n                inputs = inputs.to(device) \n                labels = labels.to(device) \n\n                optimizer.zero_grad() \n\n                with torch.set_grad_enabled(x == 'train'):\n                    preds = model(inputs) \n                    loss_value = loss(preds , labels) \n                    preds_class = preds.argmax(dim =1) \n\n                if x == 'train':\n                    loss_value.backward() \n                    optimizer.step() \n\n\n                \n                running_loss += loss_value.item() \n                running_acc += (preds_class == labels.data).float().mean() \n\n\n            epoch_loss  = running_loss / len(dataloader) \n            epoch_acc = running_acc / len(dataloader) \n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(x , epoch_loss,epoch_acc),flush = True) \n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:45.801027Z","iopub.execute_input":"2025-09-18T20:19:45.80139Z","iopub.status.idle":"2025-09-18T20:19:45.811226Z","shell.execute_reply.started":"2025-09-18T20:19:45.801364Z","shell.execute_reply":"2025-09-18T20:19:45.810345Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. prediciting PyTorch","metadata":{}},{"cell_type":"code","source":"model = models.resnet18(pretrained = True) \n\nfor y in model.parameters():\n    y.requires_grad = False \n\nmodel.fc = torch.nn.Linear(model.fc.in_features , 2) \n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device) \n\nloss = torch.nn.CrossEntropyLoss() \noptimizer = torch.optim.Adam(model.parameters(), lr = 1.0e-3)\n\nrules = torch.optim.lr_scheduler.StepLR(optimizer , step_size = 4 , gamma = 0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:45.812579Z","iopub.execute_input":"2025-09-18T20:19:45.812961Z","iopub.status.idle":"2025-09-18T20:19:46.119146Z","shell.execute_reply.started":"2025-09-18T20:19:45.812927Z","shell.execute_reply":"2025-09-18T20:19:46.117893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(model , loss ,optimizer , rules , num_epochs = 100 );","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:19:46.120561Z","iopub.execute_input":"2025-09-18T20:19:46.12094Z","iopub.status.idle":"2025-09-18T20:25:25.083791Z","shell.execute_reply.started":"2025-09-18T20:19:46.120905Z","shell.execute_reply":"2025-09-18T20:25:25.082582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Create a test file","metadata":{}},{"cell_type":"code","source":"test_uncl = 'test' \n\nshutil.copytree(os.path.join(data , 'test'),os.path.join(test_uncl,'Nene!')) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:31:21.176446Z","iopub.execute_input":"2025-09-18T20:31:21.176856Z","iopub.status.idle":"2025-09-18T20:31:21.439024Z","shell.execute_reply.started":"2025-09-18T20:31:21.176826Z","shell.execute_reply":"2025-09-18T20:31:21.436657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self,index):\n        original_tuple = super(ImageFolderWithPaths , self).__getitem__(index) \n        path = self.imgs[index][0] \n        twp = (original_tuple + (path,))\n        return twp \n\ntest_dataset = ImageFolderWithPaths('/kaggle/working/test',val_trans) \n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size = batch_size , shuffle = False , num_workers = 0\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:36:44.069146Z","iopub.execute_input":"2025-09-18T20:36:44.069558Z","iopub.status.idle":"2025-09-18T20:36:44.081843Z","shell.execute_reply.started":"2025-09-18T20:36:44.069534Z","shell.execute_reply":"2025-09-18T20:36:44.080718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:37:01.568706Z","iopub.execute_input":"2025-09-18T20:37:01.569143Z","iopub.status.idle":"2025-09-18T20:37:01.577976Z","shell.execute_reply.started":"2025-09-18T20:37:01.569111Z","shell.execute_reply":"2025-09-18T20:37:01.576365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. fixing photos test","metadata":{}},{"cell_type":"code","source":"model.eval() \n\ntest_predictions = [] \ntest_img_paths = [] \n\nfor inputs , labels , paths in tqdm(test_dataloader):\n    inputs = inputs.to(device) \n    labels = labels.to(device) \n    with torch.set_grad_enabled(False):\n        preds = model(inputs) \n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n   \n    test_img_paths.extend(paths) \n\ntest_predictions = np.concatenate(test_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:46:12.501166Z","iopub.execute_input":"2025-09-18T20:46:12.501639Z","iopub.status.idle":"2025-09-18T20:47:05.439949Z","shell.execute_reply.started":"2025-09-18T20:46:12.501612Z","shell.execute_reply":"2025-09-18T20:47:05.438468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. show image results","metadata":{}},{"cell_type":"code","source":"inputs , labels , paths = next(iter(test_dataloader)) \n\nfor img , pred in zip(inputs , test_predictions): \n   input_photos(img , title = pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T20:49:16.763678Z","iopub.execute_input":"2025-09-18T20:49:16.764053Z","iopub.status.idle":"2025-09-18T20:49:19.017978Z","shell.execute_reply.started":"2025-09-18T20:49:16.764028Z","shell.execute_reply":"2025-09-18T20:49:19.016562Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. making a new data file","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame.from_dict({'id':test_img_paths,'label':test_predictions})\nsubmission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned')\nsubmission_df['id'] = submission_df['id'].str.replace('/kaggle/working/test/Nene!','') \nsubmission_df['id'] = submission_df['id'].str.replace('.jpg','') \nsubmission_df.set_index('id',inplace = True)\nsubmission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T21:27:39.429653Z","iopub.execute_input":"2025-09-18T21:27:39.430053Z","iopub.status.idle":"2025-09-18T21:27:39.45791Z","shell.execute_reply.started":"2025-09-18T21:27:39.430024Z","shell.execute_reply":"2025-09-18T21:27:39.456412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.to_csv('submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T21:28:44.926596Z","iopub.execute_input":"2025-09-18T21:28:44.927449Z","iopub.status.idle":"2025-09-18T21:28:44.943464Z","shell.execute_reply.started":"2025-09-18T21:28:44.927386Z","shell.execute_reply":"2025-09-18T21:28:44.941853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}